{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms, models\n",
    "from  torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "# from CenterLoss import CenterLoss\n",
    "from torch.autograd.function import Function\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2020\n",
    "EPOCH = 50\n",
    "IMG_SIZE = 28\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, n_channels=1, is_train=True, transforms=None):\n",
    "        self.data = df.iloc[:, 1:].values\n",
    "#         self.fnames = df['image_id'].values\n",
    "        self.n_channels = n_channels\n",
    "        self.labels = df['label'].values\n",
    "        self.transforms = transforms\n",
    "        self.is_train = is_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx, :].reshape(IMG_SIZE, IMG_SIZE).astype(np.uint8)\n",
    "        image = (image*(255.0/image.max())).astype(np.uint8)\n",
    "        \n",
    "        if self.transforms:\n",
    "            if self.transforms.albumentations:\n",
    "                aug = Augmentation().get_augmentation(self.transforms.albumentations)\n",
    "                augmented = aug(image=image)\n",
    "                image = augmented['image'].astype(np.float32)\n",
    "\n",
    "        image = image.reshape(1, IMG_SIZE, IMG_SIZE).astype(np.float32)\n",
    "        if self.n_channels > 1:\n",
    "            image = np.concatenate([image for i in range(self.n_channels)], axis=0)\n",
    "\n",
    "        if self.is_train:\n",
    "            label = self.labels[idx]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = pd.DataFrame(index=train_df.index.values)\n",
    "folds['fold_0'] = 0\n",
    "\n",
    "fold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "for fold_, (trn_idx, val_idx) in enumerate(fold.split(train_df)):\n",
    "    folds.loc[val_idx, f'fold_{fold_}'] = 0.2\n",
    "    folds.loc[trn_idx, f'fold_{fold_}'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 0\n",
    "x_trn = train_df[folds[f'fold_{fold_num}'] > 0]\n",
    "x_val = train_df[folds[f'fold_{fold_num}'] == 0]\n",
    "\n",
    "y_val = x_val['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MnistDataset(x_trn, n_channels=1, transforms=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "valid_dataset = MnistDataset(x_val, n_channels=1, transforms=None)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### source: https://github.com/ronghuaiyang/arcface-pytorch/blob/master/models/metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "\n",
    "\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        # print(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class AddMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin cosine distance: :\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        s: norm of input feature\n",
    "        m: margin\n",
    "        cos(theta) - m\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.40):\n",
    "        super(AddMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        phi = cosine - self.m\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        # one_hot = one_hot.cuda() if cosine.is_cuda else one_hot\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        # print(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "               + 'in_features=' + str(self.in_features) \\\n",
    "               + ', out_features=' + str(self.out_features) \\\n",
    "               + ', s=' + str(self.s) \\\n",
    "               + ', m=' + str(self.m) + ')'\n",
    "\n",
    "\n",
    "class SphereProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin cosine distance: :\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        m: margin\n",
    "        cos(m*theta)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, m=4):\n",
    "        super(SphereProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.m = m\n",
    "        self.base = 1000.0\n",
    "        self.gamma = 0.12\n",
    "        self.power = 1\n",
    "        self.LambdaMin = 5.0\n",
    "        self.iter = 0\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform(self.weight)\n",
    "\n",
    "        # duplication formula\n",
    "        self.mlambda = [\n",
    "            lambda x: x ** 0,\n",
    "            lambda x: x ** 1,\n",
    "            lambda x: 2 * x ** 2 - 1,\n",
    "            lambda x: 4 * x ** 3 - 3 * x,\n",
    "            lambda x: 8 * x ** 4 - 8 * x ** 2 + 1,\n",
    "            lambda x: 16 * x ** 5 - 20 * x ** 3 + 5 * x\n",
    "        ]\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # lambda = max(lambda_min,base*(1+gamma*iteration)^(-power))\n",
    "        self.iter += 1\n",
    "        self.lamb = max(self.LambdaMin, self.base * (1 + self.gamma * self.iter) ** (-1 * self.power))\n",
    "\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        cos_theta = cos_theta.clamp(-1, 1)\n",
    "        cos_m_theta = self.mlambda[self.m](cos_theta)\n",
    "        theta = cos_theta.data.acos()\n",
    "        k = (self.m * theta / 3.14159265).floor()\n",
    "        phi_theta = ((-1.0) ** k) * cos_m_theta - 2 * k\n",
    "        NormOfFeature = torch.norm(input, 2, 1)\n",
    "\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        one_hot = torch.zeros(cos_theta.size())\n",
    "        one_hot = one_hot.cuda() if cos_theta.is_cuda else one_hot\n",
    "        one_hot.scatter_(1, label.view(-1, 1), 1)\n",
    "\n",
    "        # --------------------------- Calculate output ---------------------------\n",
    "        output = (one_hot * (phi_theta - cos_theta) / (1 + self.lamb)) + cos_theta\n",
    "        output *= NormOfFeature.view(-1, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "               + 'in_features=' + str(self.in_features) \\\n",
    "               + ', out_features=' + str(self.out_features) \\\n",
    "               + ', m=' + str(self.m) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = models.resnet18().to(device)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
    "model.fc = nn.Linear(in_features=512, out_features=512, bias=True).to(device)\n",
    "\n",
    "metric_fc = ArcMarginProduct(512, NUM_CLASSES, s=30, m=0.5, easy_margin=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "sheduler = lr_scheduler.StepLR(optimizer, 20, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Epoch 1 - avg_train_loss: 7.2479  avg_val_loss: 1.5867 val_score: 0.8597 time: 3s<p>Epoch 2 - avg_train_loss: 1.0237  avg_val_loss: 1.1922 val_score: 0.9018 time: 3s<p>Epoch 3 - avg_train_loss: 0.5589  avg_val_loss: 1.5770 val_score: 0.8866 time: 3s<p>Epoch 4 - avg_train_loss: 0.3618  avg_val_loss: 0.9052 val_score: 0.9340 time: 3s<p>Epoch 5 - avg_train_loss: 0.1915  avg_val_loss: 0.8154 val_score: 0.9398 time: 3s<p>Epoch 6 - avg_train_loss: 0.1427  avg_val_loss: 0.8750 val_score: 0.9406 time: 3s<p>Epoch 7 - avg_train_loss: 0.0800  avg_val_loss: 0.7785 val_score: 0.9471 time: 3s<p>Epoch 8 - avg_train_loss: 0.0798  avg_val_loss: 0.7735 val_score: 0.9468 time: 3s<p>Epoch 9 - avg_train_loss: 0.0381  avg_val_loss: 0.6537 val_score: 0.9554 time: 3s<p>Epoch 10 - avg_train_loss: 0.0390  avg_val_loss: 0.7939 val_score: 0.9507 time: 3s<p>Epoch 11 - avg_train_loss: 0.0471  avg_val_loss: 0.7097 val_score: 0.9522 time: 3s<p>Epoch 12 - avg_train_loss: 0.0266  avg_val_loss: 0.6861 val_score: 0.9549 time: 3s<p>Epoch 13 - avg_train_loss: 0.0281  avg_val_loss: 0.7191 val_score: 0.9524 time: 3s<p>Epoch 14 - avg_train_loss: 0.0337  avg_val_loss: 0.6884 val_score: 0.9561 time: 3s<p>Epoch 15 - avg_train_loss: 0.0210  avg_val_loss: 0.6369 val_score: 0.9597 time: 3s<p>Epoch 16 - avg_train_loss: 0.0151  avg_val_loss: 0.6524 val_score: 0.9591 time: 3s<p>Epoch 17 - avg_train_loss: 0.0093  avg_val_loss: 0.6305 val_score: 0.9606 time: 3s<p>Epoch 18 - avg_train_loss: 0.0226  avg_val_loss: 0.6922 val_score: 0.9572 time: 3s<p>Epoch 19 - avg_train_loss: 0.0156  avg_val_loss: 0.6799 val_score: 0.9592 time: 3s<p>Epoch 20 - avg_train_loss: 0.0083  avg_val_loss: 0.6280 val_score: 0.9604 time: 3s<p>Epoch 21 - avg_train_loss: 0.0042  avg_val_loss: 0.6086 val_score: 0.9630 time: 3s<p>Epoch 22 - avg_train_loss: 0.0079  avg_val_loss: 0.6173 val_score: 0.9621 time: 3s<p>Epoch 23 - avg_train_loss: 0.0108  avg_val_loss: 0.6408 val_score: 0.9608 time: 3s<p>Epoch 24 - avg_train_loss: 0.0208  avg_val_loss: 0.6915 val_score: 0.9587 time: 3s<p>Epoch 25 - avg_train_loss: 0.0110  avg_val_loss: 0.5919 val_score: 0.9627 time: 3s<p>Epoch 26 - avg_train_loss: 0.0039  avg_val_loss: 0.6014 val_score: 0.9629 time: 3s<p>Epoch 27 - avg_train_loss: 0.0034  avg_val_loss: 0.6105 val_score: 0.9621 time: 3s<p>Epoch 28 - avg_train_loss: 0.0052  avg_val_loss: 0.6410 val_score: 0.9613 time: 3s<p>Epoch 29 - avg_train_loss: 0.0069  avg_val_loss: 0.6032 val_score: 0.9631 time: 3s<p>Epoch 30 - avg_train_loss: 0.0083  avg_val_loss: 0.6556 val_score: 0.9606 time: 3s<p>Epoch 31 - avg_train_loss: 0.0097  avg_val_loss: 0.5903 val_score: 0.9636 time: 3s<p>Epoch 32 - avg_train_loss: 0.0020  avg_val_loss: 0.6107 val_score: 0.9627 time: 3s<p>Epoch 33 - avg_train_loss: 0.0130  avg_val_loss: 0.6229 val_score: 0.9617 time: 3s<p>Epoch 34 - avg_train_loss: 0.0037  avg_val_loss: 0.5975 val_score: 0.9635 time: 3s<p>Epoch 35 - avg_train_loss: 0.0050  avg_val_loss: 0.6318 val_score: 0.9620 time: 3s<p>Epoch 36 - avg_train_loss: 0.0043  avg_val_loss: 0.5887 val_score: 0.9651 time: 3s<p>Epoch 37 - avg_train_loss: 0.0034  avg_val_loss: 0.5870 val_score: 0.9651 time: 3s<p>Epoch 38 - avg_train_loss: 0.0005  avg_val_loss: 0.5862 val_score: 0.9652 time: 3s<p>Epoch 39 - avg_train_loss: 0.0006  avg_val_loss: 0.5859 val_score: 0.9648 time: 3s<p>Epoch 40 - avg_train_loss: 0.0042  avg_val_loss: 0.5754 val_score: 0.9653 time: 3s<p>Epoch 41 - avg_train_loss: 0.0032  avg_val_loss: 0.5648 val_score: 0.9650 time: 3s<p>Epoch 42 - avg_train_loss: 0.0035  avg_val_loss: 0.6272 val_score: 0.9629 time: 3s<p>Epoch 43 - avg_train_loss: 0.0046  avg_val_loss: 0.6016 val_score: 0.9638 time: 3s<p>Epoch 44 - avg_train_loss: 0.0076  avg_val_loss: 0.5966 val_score: 0.9639 time: 3s<p>Epoch 45 - avg_train_loss: 0.0041  avg_val_loss: 0.6172 val_score: 0.9625 time: 3s<p>Epoch 46 - avg_train_loss: 0.0023  avg_val_loss: 0.5861 val_score: 0.9653 time: 3s<p>Epoch 47 - avg_train_loss: 0.0062  avg_val_loss: 0.6705 val_score: 0.9621 time: 3s<p>Epoch 48 - avg_train_loss: 0.0135  avg_val_loss: 0.6228 val_score: 0.9630 time: 3s<p>Epoch 49 - avg_train_loss: 0.0102  avg_val_loss: 0.6480 val_score: 0.9625 time: 3s<p>Epoch 50 - avg_train_loss: 0.0145  avg_val_loss: 0.6851 val_score: 0.9599 time: 3s"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "CV: 0.9653062251567915\n",
      "\n",
      "BEST EPOCH: 40\n",
      "BEST RECALL: 0.9653062251567915\n",
      "\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_epoch = -1\n",
    "best_val_score = -np.inf\n",
    "mb = master_bar(range(EPOCH))\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "val_score_list = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for epoch in mb:\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "\n",
    "    for images, labels in progress_bar(train_loader, parent=mb):\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        \n",
    "        feature = model(images.float())\n",
    "        preds = metric_fc(feature, labels)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "    train_loss_list.append(avg_loss)\n",
    "\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    valid_preds = np.zeros((len(valid_loader.dataset), NUM_CLASSES))\n",
    "    avg_val_loss = 0.\n",
    "\n",
    "    for i, (images, labels) in enumerate(valid_loader):\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "\n",
    "        feature = model(images.float())\n",
    "        preds = metric_fc(feature, labels)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        valid_preds[i * BATCH_SIZE: (i + 1) * BATCH_SIZE] = preds.cpu().detach().numpy()\n",
    "        avg_val_loss += loss.item() / len(valid_loader)\n",
    "\n",
    "    val_score = recall_score(y_val, np.argmax(valid_preds, axis=1), average='macro')\n",
    "\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    val_score_list.append(val_score)\n",
    "        \n",
    "    elapsed = time.time() - start_time\n",
    "    mb.write(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f} val_score: {val_score:.4f} time: {elapsed:.0f}s')\n",
    "\n",
    "    if best_val_score < val_score:\n",
    "        best_epoch = epoch + 1\n",
    "        best_val_score = val_score\n",
    "        best_valid_preds = valid_preds\n",
    "        best_model = model.state_dict()\n",
    "        counter = 0\n",
    "        \n",
    "    counter += 1\n",
    "    if counter == 100:\n",
    "        break\n",
    "\n",
    "print('\\n\\n===================================\\n')\n",
    "print(f'CV: {best_val_score}\\n')\n",
    "print(f'BEST EPOCH: {best_epoch}')\n",
    "print(f'BEST RECALL: {best_val_score}')\n",
    "print('\\n===================================\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
